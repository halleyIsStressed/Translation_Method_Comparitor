{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["CcA4CAK695xY"],"authorship_tag":"ABX9TyNqLfCZ93pIvxU9GQb7r/Du"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"5UdPheHn92so"}},{"cell_type":"markdown","source":["# Combined Translation Colab UI\n","Importing all trained tokenizers and models (if exists) and compare it with one another, and also Google Translate.\n","\n","Takes around 2 minutes"],"metadata":{"id":"CcA4CAK695xY"}},{"cell_type":"code","source":["!pip install transformers sentencepiece torch\n","!pip install deep-translator\n","!pip install sacrebleu\n","!pip install --upgrade googletrans\n","!pip install https://github.com/kpu/kenlm/archive/master.zip #kenlm"],"metadata":{"id":"lWKdtLeC915q"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbwcTSY49xxP"},"outputs":[],"source":["from transformers import MarianMTModel, MarianTokenizer\n","from transformers import pipeline\n","from google.colab import files\n","from google.colab import drive\n","from googletrans import Translator as gt\n","from deep_translator import GoogleTranslator\n","from nltk.stem import WordNetLemmatizer\n","from collections import defaultdict\n","\n","import itertools\n","import sacrebleu\n","import requests\n","import kenlm\n","import nltk\n","import math\n","import json\n","import re\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')"]},{"cell_type":"code","source":["# Mounting drive file\n","drive.mount('/content/data')"],"metadata":{"id":"_EIiwbizC7IB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757045890698,"user_tz":-480,"elapsed":19237,"user":{"displayName":"LAP YHIN LEE","userId":"11000066899309620827"}},"outputId":"7fe4bf13-638b-49c1-8099-ddf2c42554d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/data\n"]}]},{"cell_type":"code","source":["# Load JSON -> Dictionary\n","import json\n","\n","def load_dictionary(file_path):\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        raw_data = json.load(f)\n","\n","    dictionary = {}\n","    for entry in raw_data:\n","        eng = entry.get(\"word\", \"\").strip().lower()\n","        trans_list = entry.get(\"translation\", [])\n","        if isinstance(trans_list, list) and len(trans_list) > 0:\n","            dictionary[eng] = trans_list[0]\n","    return dictionary\n","\n","dict_path = \"/content/data/MyDrive/AI Assignment Colab/data/filtered.json\"\n","dictionary = load_dictionary(dict_path)\n","print(f\"✅ Dictionary loading success, total number of entries: {len(dictionary)}\")\n","\n","# 3. Clean the translated text\n","\n","def _strip_brackets(s: str) -> str:\n","    s = re.sub(r\"【[^】]*】\", \"\", s)\n","    s = re.sub(r\"\\([^)]*\\)\", \"\", s)\n","    s = re.sub(r\"（[^）]*）\", \"\", s)\n","    s = re.sub(r\"\\[[^\\]]*\\]\", \"\", s)\n","    return s\n","\n","def clean_translation_text(text):\n","    if isinstance(text, list):\n","        text = text[0] if text else \"\"\n","    if not isinstance(text, str):\n","        return \"\"\n","    candidates = re.split(r\"[;,；，、/]| or \", text)\n","    pos_pat = re.compile(\n","        r\"^(?:n|v|vi|vt|adj|adv|prep|conj|pron|abbr|int|interj|art|aux|num|det|modal|phr|idiom)\\.\\s*\",\n","        re.IGNORECASE\n","    )\n","    for cand in candidates:\n","        cand = pos_pat.sub(\"\", cand.strip())\n","        cand = _strip_brackets(cand).strip()\n","        if re.search(r\"[\\u4e00-\\u9fff]\", cand):\n","            return cand\n","    return pos_pat.sub(\"\", candidates[0].strip()) if candidates else \"\"\n","\n","def lookup(word: str) -> str:\n","    word = word.lower()\n","    raw = dictionary.get(word, \"\")\n","    return clean_translation_text(raw)\n","\n","# NLTK Initialisation & Participle + Word Form Reduction\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","def analyze(sentence):\n","    sentence = sentence.lower()\n","    sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence)\n","    words = sentence.split()\n","    words = [lemmatizer.lemmatize(w, pos='v') for w in words]\n","    words = [lemmatizer.lemmatize(w, pos='n') for w in words]\n","    return words\n","\n","# RBMT generates translation (最新版)\n","\n","# -----------------------------\n","# 规则字典\n","# -----------------------------\n","subjects = {\"i\":\"我\",\"you\":\"你\",\"he\":\"他\",\"she\":\"她\",\"we\":\"我们\",\"they\":\"他们\",\"it\":\"它\"}\n","possessives = {\"my\":\"我的\",\"your\":\"你的\",\"his\":\"他的\",\"her\":\"她的\",\"their\":\"他们的\",\"our\":\"我们的\"}\n","verbs = {\"eat\":\"吃\",\"drink\":\"喝\",\"go\":\"去\",\"play\":\"玩\",\"like\":\"喜欢\",\"see\":\"看\",\"know\":\"知道\",\"say\":\"说\",\"love\":\"爱\",\"want\":\"想\",\"do\":\"做\",\"make\":\"做/制作\",\"take\":\"拿\",\"get\":\"得到\",\"have\":\"有\",\"check\":\"检查\",\"ensure\":\"确定\"}\n","adjectives = {\"happy\":\"快乐\",\"total\":\"全体的\",\"maximum\":\"极点\",\"specific\":\"特定\"}\n","time_words = {\"today\":\"今天\",\"tomorrow\":\"明天\",\"yesterday\":\"昨天\",\"now\":\"现在\",\"morning\":\"早上\",\"afternoon\":\"下午\",\"evening\":\"晚上\",\"then\":\"然后\"}\n","connectors = {\"and\":\"和\",\"or\":\"或\",\"but\":\"但是\",\"to\":\"\",\"for\":\"为\",\"in\":\"在\"}\n","be_verbs = {\"am\":\"是\",\"is\":\"是\",\"are\":\"是\",\"was\":\"是\",\"were\":\"是\",\"be\":\"是\"}\n","negations = {\"not\":\"不\",\"don't\":\"不\",\"doesn't\":\"不\",\"didn't\":\"没\"}\n","question_words = {\"what\":\"什么\",\"who\":\"谁\",\"where\":\"哪里\",\"when\":\"什么时候\",\"why\":\"为什么\",\"how\":\"怎么\"}\n","others = {\"ok\":\"好\",\"nice\":\"美好\",\"hi\":\"嗨\",\"hello\":\"你好\",\"friend\":\"朋友\",\"book\":\"书\",\"phone\":\"电话\",\"seat\":\"座\",\"availability\":\"有效性\",\"status\":\"状态\",\"capacity\":\"容量\",\"class\":\"班级\",\"verifying\":\"证明\"}\n","\n","# -----------------------------\n","# 固定搭配短语\n","# -----------------------------\n","fixed_combinations = {\n","    (\"thank\",\"you\"):\"谢谢\",\n","    (\"thanks\",):\"谢谢\",\n","    (\"good\",\"morning\"):\"早上好\",\n","    (\"good\",\"afternoon\"):\"下午好\",\n","    (\"good\",\"evening\"):\"晚上好\",\n","    (\"good\",\"night\"):\"晚安\",\n","    (\"be\",\"ok\"):\"很好\",\n","    (\"play\",\"game\"):\"玩游戏\",\n","    (\"eat\",\"apple\"):\"吃苹果\",\n","    (\"drink\",\"water\"):\"喝水\",\n","    (\"how\",\"are\",\"you\"):\"你好吗\",\n","    (\"see\",\"you\"):\"再见\",\n","    (\"year\",\"old\"):\"岁\"\n","}\n","\n","# -----------------------------\n","# 缩写字典\n","# -----------------------------\n","contractions = {\n","    \"it's\":\"it is\",\n","    \"i'm\":\"i am\",\n","    \"you're\":\"you are\",\n","    \"we're\":\"we are\",\n","    \"they're\":\"they are\",\n","    \"i've\":\"i have\",\n","    \"you've\":\"you have\",\n","    \"isn't\":\"is not\",\n","    \"aren't\":\"are not\",\n","    \"don't\":\"do not\",\n","    \"doesn't\":\"does not\",\n","}\n","\n","ignore_words = [\"to\", \"a\", \"an\", \"the\"]  # 冠词/虚词忽略\n","\n","def expand_contractions(sentence):\n","    for k,v in contractions.items():\n","        sentence = re.sub(r\"\\b\"+k+r\"\\b\", v, sentence, flags=re.IGNORECASE)\n","    return sentence\n","\n","# -----------------------------\n","# 载入 JSON dataset\n","# -----------------------------\n","dict_path = \"/content/data/MyDrive/AI Assignment Colab/data/filtered.json\"  # 改成你的路径\n","with open(dict_path, \"r\", encoding=\"utf-8\") as f:\n","    raw_data = json.load(f)\n","\n","dataset_dict = {}\n","for entry in raw_data:\n","    eng = entry.get(\"word\",\"\").strip().lower()\n","    trans_list = entry.get(\"translation\", [])\n","    if isinstance(trans_list, list) and len(trans_list) > 0:\n","        dataset_dict[eng] = trans_list[0]\n","\n","# -----------------------------\n","# clean translation\n","# -----------------------------\n","def clean_translation_text(text):\n","    if isinstance(text, list):\n","        text = text[0] if text else \"\"\n","    if not isinstance(text, str):\n","        return \"\"\n","    candidates = re.split(r\"[;,；，、/]| or \", text)\n","    for cand in candidates:\n","        cand = re.sub(r\"^(?:n|v|vi|vt|adj|adv|prep|conj|pron|abbr|int|interj|art|aux|num|det|modal|phr|idiom)\\.\\s*\", \"\", cand, flags=re.IGNORECASE)\n","        cand = re.sub(r\"【[^】]*】|\\([^)]*\\)|（[^）]*）|\\[[^\\]]*\\]\", \"\", cand).strip()\n","        if re.search(r\"[\\u4e00-\\u9fff]\", cand):\n","            return cand\n","    return candidates[0].strip() if candidates else \"\"\n","\n","# -----------------------------\n","# lookup 函数\n","# -----------------------------\n","def lookup(word):\n","    lw = word.lower()\n","    if lw in ignore_words:\n","        return \"\"   # 冠词忽略\n","\n","    for dic in [subjects, possessives, verbs, adjectives, time_words, connectors, be_verbs, negations, question_words, others]:\n","        if lw in dic:\n","            return dic[lw]\n","\n","    if lw in dataset_dict:\n","        trans = dataset_dict[lw]\n","        return clean_translation_text(trans)\n","\n","    # fallback，保证永远返回完整中文\n","    return \"未知词\"\n","\n","# -----------------------------\n","# 预处理句子，去掉标点，分词\n","# -----------------------------\n","def preprocess_sentence(sentence):\n","    # 统一替换缩写\n","    sentence = expand_contractions(sentence)\n","    # 去掉所有标点符号，包括句号、逗号等\n","    sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence)\n","    # 小写化\n","    sentence = sentence.lower()\n","    # 分词\n","    words = sentence.strip().split()\n","    # 忽略冠词/虚词\n","    words = [w for w in words if w not in ignore_words]\n","    return words\n","\n","\n","# -----------------------------\n","# generate 函数（接收完整句子）\n","# -----------------------------\n","def generate_translation(sentence):\n","    words = preprocess_sentence(sentence)\n","    result = []\n","    i = 0\n","    while i < len(words):\n","        w = words[i].lower()\n","        if w in ignore_words:\n","            i += 1\n","            continue\n","\n","        # 三词固定搭配\n","        if i+2 < len(words) and (words[i].lower(), words[i+1].lower(), words[i+2].lower()) in fixed_combinations:\n","            result.append(fixed_combinations[(words[i].lower(), words[i+1].lower(), words[i+2].lower())])\n","            i += 3\n","            continue\n","\n","        # 两词固定搭配\n","        if i+1 < len(words) and (w, words[i+1].lower()) in fixed_combinations:\n","            result.append(fixed_combinations[(w, words[i+1].lower())])\n","            i += 2\n","            continue\n","\n","        # 时间词\n","        if w in time_words:\n","            result.append(time_words[w])\n","            i += 1\n","            continue\n","\n","        # 疑问词\n","        if w in question_words:\n","            result.append(question_words[w])\n","            i += 1\n","            continue\n","\n","        # 主语\n","        if w in subjects:\n","            result.append(subjects[w])\n","            i += 1\n","            continue\n","\n","        # be 动词\n","        if w in be_verbs:\n","            if i+1 < len(words) and words[i+1].lower() == \"ok\":\n","                result.append(\"很好\")\n","                i += 2\n","                continue\n","            if i+1 < len(words):\n","                nxt = words[i+1].lower()\n","                if nxt in adjectives:\n","                    result.append(\"很\" + adjectives[nxt])\n","                    i += 2\n","                    continue\n","            result.append(be_verbs[w])\n","            i += 1\n","            continue\n","\n","        # 否定\n","        if w in negations:\n","            result.append(negations[w])\n","            i += 1\n","            continue\n","\n","        # 所有格 + 名词\n","        if w in possessives and i+1 < len(words):\n","            nxt = words[i+1].lower()\n","            obj = lookup(nxt)\n","            if obj:\n","                result.append(possessives[w] + obj)\n","                i += 2\n","                continue\n","\n","        # 动词 + 宾语\n","        if w in verbs:\n","            verb_trans = verbs[w]\n","            obj = \"\"\n","            if i+1 < len(words):\n","                nxt = words[i+1].lower()\n","                if nxt not in connectors and nxt not in ignore_words:\n","                    obj = lookup(nxt)\n","                    if obj:\n","                        result.append(verb_trans + obj)\n","                        i += 2\n","                        continue\n","            result.append(verb_trans)\n","            i += 1\n","            continue\n","\n","        # 形容词\n","        if w in adjectives:\n","            result.append(adjectives[w])\n","            i += 1\n","            continue\n","\n","        # 连词\n","        if w in connectors:\n","            result.append(connectors[w])\n","            i += 1\n","            continue\n","\n","        # fallback\n","        trans = lookup(w)\n","        if trans:\n","            result.append(trans)\n","        i += 1\n","\n","    return \"\".join(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yxky65V2ec8e","executionInfo":{"status":"ok","timestamp":1757045932171,"user_tz":-480,"elapsed":39391,"user":{"displayName":"LAP YHIN LEE","userId":"11000066899309620827"}},"outputId":"d8bd522a-8a1a-4d5f-a14a-8a2f0602b4c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Dictionary loading success, total number of entries: 3020788\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}]},{"cell_type":"code","source":["input_sentence = \"\"\n","\n","# Loading the NMT model\n","nmtTokenizer = MarianTokenizer.from_pretrained(\"./data/MyDrive/AI Assignment Colab/data/fine_tuned_marian\")\n","nmtModel = MarianMTModel.from_pretrained(\"./data/MyDrive/AI Assignment Colab/data/fine_tuned_marian\")\n","\n","# Loading the SMT model\n","model = kenlm.Model('./data/MyDrive/AI Assignment Colab/data/Chinese_LM.arpa')\n","alpha = 1\n","beta = 1\n","with open(\"./data/MyDrive/AI Assignment Colab/data/En_Cn_Probs.json\", \"r\", encoding=\"utf-8\") as f:\n","    en_cn_probs = json.load(f)\n","\n","def translate_word(english_word, probability_threshold=0.0):\n","    english_word = english_word.lower()\n","\n","    with open(\"./data/MyDrive/AI Assignment Colab/data/En_Cn_Probs.json\", \"r\", encoding=\"utf-8\") as f:\n","        word_probabilities = json.load(f)\n","\n","    translations = {\n","        zh_word: prob\n","        for zh_word, prob in word_probabilities.get(english_word, {}).items()\n","        if prob >= probability_threshold\n","    }\n","\n","    return dict(sorted(translations.items(), key=lambda item: item[1], reverse=True))\n","\n","def sentence_decode(eng_sentence):\n","    tokens = nltk.word_tokenize(eng_sentence.lower())\n","\n","    # Generate all possible translation combinations\n","    u_tokens = sorted(set(tokens), key=tokens.index)\n","    translations = {token: list(translate_word(token).keys()) for token in u_tokens}\n","\n","    # Generate combinations of Chinese words\n","    translation_options = [translations.get(token, []) for token in u_tokens]\n","    translation_iterations = {\" \".join(combination) for combination in itertools.product(*translation_options)}\n","\n","    # Handle repeated words by creating variations with fewer duplicates\n","    new_iterations = set()\n","    for sentence in translation_iterations:\n","        words = sentence.split()\n","        counts = defaultdict(int)\n","        for word in words:\n","            counts[word] += 1\n","        for word, count in counts.items():\n","            if count > 1:\n","                new_words = list(words)\n","                new_words.remove(word)\n","                new_iterations.add(\" \".join(new_words))\n","\n","    translation_iterations |= new_iterations\n","\n","    # Save all translation candidates\n","    with open(\"Trans_Chinese_Iter.txt\", \"w\", encoding=\"utf-8\") as f:\n","        for i, sentence in enumerate(sorted(translation_iterations), 1):\n","            f.write(f\"{sentence}\\n\")\n","\n","    # Score candidates with KenLM + word translation probabilities\n","    scored_sentences = []\n","    with open(\"Trans_Iter_Score.txt\", \"w\", encoding=\"utf-8\") as f:\n","        for sentence in sorted(translation_iterations):\n","            words = sentence.split()\n","\n","            translation_prob = 1.0\n","            for en_word, zh_word in zip(u_tokens, words):\n","                prob = en_cn_probs.get(en_word, {}).get(zh_word, 0.0)\n","                translation_prob *= prob if prob > 0 else 1e-9\n","            log_prob = math.log(translation_prob)\n","\n","            lm_score = model.score(sentence)\n","            final_score = alpha * log_prob + beta * lm_score\n","            scored_sentences.append((sentence, final_score))\n","\n","            f.write(f\"{sentence}\\t{final_score:.6f}\\n\")\n","\n","    # Print top 5 by final score\n","    top_5 = sorted(scored_sentences, key=lambda x: x[1], reverse=True)[:1]\n","    for sentence, score in top_5:\n","        return sentence.replace(\" \", \"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2q5NCTvsDOfi","executionInfo":{"status":"ok","timestamp":1757046014106,"user_tz":-480,"elapsed":43299,"user":{"displayName":"LAP YHIN LEE","userId":"11000066899309620827"}},"outputId":"1d0228c2-c472-4e4d-ffa5-d7dbb1cf23c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n"]}]},{"cell_type":"markdown","source":["# Translation Part"],"metadata":{"id":"5nYwRl4PVGeC"}},{"cell_type":"code","source":["input_sentence = input(\"Please enter an English sentence:\")\n","reference = input(\"Enter the Reference Translation: \")\n","print(\"\")\n","\n","rbmt_input = input_sentence\n","smt_input = input_sentence\n","nmt_input = input_sentence\n","google_input = input_sentence\n","\n","# Running RBMT Translator\n","def split_into_clauses(sentence):\n","    clauses = re.split(r'[,.!?;]| and | or ', sentence)\n","    return [c.strip() for c in clauses if c.strip()]\n","\n","rbmt_input = expand_contractions(rbmt_input)\n","words = analyze(rbmt_input)\n","\n","if len(words) <= 8:\n","  rbmt_output = generate_translation(rbmt_input)\n","else:\n","  clauses = split_into_clauses(rbmt_input)\n","  translated_clauses = [generate_translation(c) for c in clauses]\n","  rbmt_output = \"，\".join(translated_clauses)\n","\n","print(f\"RBMT Output\\t: {rbmt_output}\")\n","\n","\n","# Running SMT Translator\n","smt_input = re.sub(r'[^\\w\\s]', '', smt_input)\n","print(f\"SMT Output\\t: {sentence_decode(smt_input)}\")\n","\n","\n","# Running NMT Translator\n","inputs = nmtTokenizer(nmt_input, return_tensors=\"pt\", padding=True, truncation=True)\n","translated_tokens = nmtModel.generate(**inputs)\n","nmt_output = nmtTokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n","print(f\"NMT Output\\t: {nmt_output}\")\n","\n","\n","# Running Google Translator\n","google_translator = gt()\n","google_output = await google_translator.translate(google_input, dest=\"zh-cn\")\n","print(f\"Google Output\\t: {google_output.text}\")\n","\n","rbmt_bleu   = sacrebleu.sentence_bleu(rbmt_output, [reference], tokenize='zh')\n","smt_bleu    = sacrebleu.sentence_bleu(sentence_decode(smt_input), [reference], tokenize='zh')\n","nmt_bleu    = sacrebleu.sentence_bleu(nmt_output, [reference], tokenize='zh')\n","google_bleu = sacrebleu.sentence_bleu(google_output.text, [reference], tokenize='zh')\n","\n","print(f\"\\nRBMT BLEU \\t: {rbmt_bleu.score:.2f}\")\n","print(f\"SMT BLEU \\t: {smt_bleu.score:.2f}\")\n","print(f\"NMT BLEU \\t: {nmt_bleu.score:.2f}\")\n","print(f\"Google BLEU \\t: {google_bleu.score:.2f}\")\n"],"metadata":{"id":"CNzS8VqvSVQ6"},"execution_count":null,"outputs":[]}]}